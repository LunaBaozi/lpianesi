---
title: 'On LLMs and technology as more than tools'
date: 2025-07-21
permalink: /posts/2025/07/blog-post-2/
layout: single
toc: true
tags:
  - writing
  - science
  - llms
--- 

**16.10.2025 This blog post is a work in progress!**

I am writing my first ever paper, and I am of course completely lost.

It is an incredibly difficult task, no matter how many years you already spent reading and reviewing other people's papers.

But while going through this exhausting process, I gained clarity about something: I will use LLMs, and I will be transparent about it.

Let's break this down a bit and let's be honest with ourselves: everyone uses LLMs somehow in their work. Be it in the form of coding assistants, literature review assistants, correction of a text written in your second language, general questions...and so on and so forth. I am no special in that sense, because I started using LLMs extensively about two months ago. To be completely honest, I hardly ever used any chatbot before that; wrongly so, I had a quite negative impression of them. 

There are a few things I would like to discuss about in this post, and I will try my best to elaborate on them so that they can result in a coherent picture of the disorganized thoughts I have in mind:
1. Coding is not an easy task for me
2. The wrong understanding of technology as merely an instrument
3. How and how much should we incorporate LLMs into our routine work


## Coding is hard
For the longest time, I had a love-hate relationship with coding. I've always been a book-type-of-kid, and later on studied latin and ancient greek in high school. So for the most part, I didn't have a good impression of technology - books grew me up with a deep fascination for intricate thoughts, romantic feelings, and in general the pleasure of enjoying things in their slowness. But at the same time, I would spend hours as a kid on my dad's super old cathodic tube computer playing videogames me and my sister found in cereal boxes, and later on, sitting besides my dad working on his desk, on a little space he carved for me, on his other old (not as old as the first one) laptop, where I dived deep into YouTube to listen to my favourite artists' songs and learn them by heart, or watch TV series my friends recommended to me. 

So I guess I've always been fascinated by technology, I just didn't fully realize it until during my Bachelor's. There, I struggled A LOT with my first programming exam.

I remember having a full on meltdown at that time, and talking with my friends about how I found programming "sterile", "robotic", and "lacking any meaning" (gosh, sometimes I really could be an arrogant kid). It was only until later during my program, when I attended some courses about statistics (we used R), molecular dynamics (we used simulation softwares), and bioinformatics (we used the terminal and a lot of tools), that I started to appreciate and actually get passionate about all the possibilities that could open when you start using computers. So then I convinced myself I needed to improve, and went on to enroll in an artificial intelligence MSc program. Let's elegantly glide over how hard that was, again, but still, I was now completely caught in the fascination, and that luckily never stopped.

Fascination and passion for something does not imply being good at it, so my struggle with coding still exists. It's just something that doesn't come easy for me, I know that, and it's fine. But it means that sometimes I struggle so much, that I end in a total executive dysfunction [^execdys] episode, which can last for weeks, or even months. Several other external factors have also contributed and still contribute to the occurrence of these events, but let's focus on my personal struggle here. I've always thought that I wasn't "good enough" for programming, that I needed to push myself even harder, or, sometimes, that doing science just wasn't for me, afterall; being in this mindset is such a destructive move for your motivation (expecially during your PhD), and coming to terms with your limits, and recognizing that they are not a quality evaluation of your work nor your person, can really put you in a different perspective.

There's another self-sabotaging mechanism that I've always acted on myself, which is, unsurprisingly, exclusively sabotaging: the idea that if you don't struggle enough when doing something, the result won't be worth it, somehow coupled with the belief that you should be immediately good at something, otherwise that thing is simply not your thing. I guess a lot of PhD students can relate to these "two wolves living inside of you".

![Two wolves meme](/images/wolves.jpeg)

To my utter surprise and delight, when I gave in to the idea that I simply didn't need to struggle that hard, and started looking for ways to make my life easier, I found LLM coding agents, and they **worked wonders**---who would've thought!

So now, I jokingly say to my friends that I have "a new supervisor", and show them my shiny new pro subscription plan to Claude.

In the past two months, I've been extensively using Claude, ChatGPT and DeepSeek: 99% of the times, as a coding assistant or conversational agent about coding matters; 1% of the times for other tasks, like content summarization or literature review. I found Claude to be the nicest for my needs, with DeepSeek coming in second place, and ChatGPT in third (based on a subjective and non-rigorous evaluation of the number of hallucinations and the coherence of the response with the request). After this trial period, I can confidently say that coding is not a burden for me anymore, and I actually truly enjoy it. 

So, how did using LLMs changed my experience with programming so radically? It gave me a way to crack two of my personal hardest nuts: initiating tasks, and the fear of asking questions. About the first one, I can say that because I don't have a proper training in computer science from the grounds up, I often fail to break down a problem into pieces and recognize what to start with; about the second, it's partly a personality trait, and partly due to the constant feeling of asking stupid questions because... -> go back to the first point. As many other people have pointed out already (CITATION MISSING), conversing with LLMs as a student almost feels like talking with an infinitely patient and non-judging mentor. I can definitely agree with this feeling.

Consideration which directly leads me to the next paragraph: is technology really *only* a tool?


## Technology as more than a tool
I'll use an extract from a book I'm recently reading, and about which I talked at the end of [this](https://lunabaozi.github.io/lpianesi/posts/2025/06/blog-post-1/) previous post.

"Often, we forget that the *non human* machine is not a passive object, but plays an important role in determining the behaviours of the *human* called *user*. The digital machine connected to the internet is not only the object-smartphone we hold in our hands: it's a software [...]; it's a hardware... [...] An extraordinary complexity that almost evaporates in front our sensations, unique and unrepeatable, yet so repeated and ordinary: anxiety, boredom, irritation, frustration, expectation, enthusiasm, excitement, and more. Observing more attentively instead, while keeping an eye towards what happens inside our body, we will be more and more aware that the machine (software and hardware) is an important actor in constructing emotional experiences, able to cause pre-determined re-actions, sometimes invasively and pressingly too. All of this does not happen by chance, but *by design*, depending on how the technical system has been designed and implemented. In training proposals, especially those targeted to the younger population, it is often reiterated that digital devices are only 'tools' and thus [their effect] depends on how you use them: if you use them well, you'll obtain a certain result, if you use them badly, something bad will happen (implied, it will all be your fault!). We propose a different narration. Every application and platform has its own character, we call it daimon in Greek, demon in English, meaning an internal voice that guides systems, a strongly present push in interactions; it's not necessarily evil, even though the interaction can result toxic for us. If we underestimate its presence, we will give a certain omnipotency to human will and, in the case of educational contexts, to the young people's will. We assume that the individual has the full control of their own acting, ignoring the vulnerabilities, the idiosyncrasies, the unspoken and acted mechanisms, in a word, everything that happens on the plane of un-consciousness, of behavioural automatism and diffused as well as irreflected habit, previleged territories in which these devices operate. Moving instead from the fact that we are not facing neutral objects will open crucial questions: what is the character of this application? What does it favour? What does it discourage? What will incourage me to do? What will be my vulnerabilities on which it will build to put forward its 'intentions'? " [^hacker] (Possibly inaccurate translation from Italian by me).

The one of machines having a "demon" inside is a very interesting perspective and the first time I came across something like this in science. To be completely honest, before reading that book I was also believing that digital devices were "only" a tool, even though there was something that never fit quite right with it; reading this different perspective allowed me to expand my view on the matter.

Admitting that the machines we use are *by design* shaping our thoughts and the way we interact with them, opens a deep, deep rabbit hole. *Someone* is commissioning their creation, *someone* is designing their interface, *someone* is implementing the software, *someone* is building them. With this, I don't want to sound like a conspiracy theorist, but I want to highlight that these *tools* are not magical, but are thought and built with certain intentions, thus possess a steering power on the user.

This is strictly tied to our activity as scientists: the science we produce in the form of papers, data, models, software, *will* have a certain impact on society. The decision behind what topic to research and which findings about it to subsequently publish has a considerable weight, I would also dare say a political weight. I don't believe in hiding behind the mask of science being objective and unbiased, because it simply is not; many events have given us proof of it, with equally as many people talking about different aspects of the matter [^women].

Now that I've given some structure to my thoughts, talked about how helpful LLMs have become to me, and how we can view technology as an integral aspect of our lives that shapes and modifies what we think, the natural question is: where do we set the limit?


## The tradeoff between giving up agency and speeding up your work
In an interesting career column on Nature about the use of LLMs in the peer review process [^nature1], the authors raise an interesting point: "Every time a scientist abdicates their work to an AI tool, that is a tacit admission that the work is not worth being done by the scientist". While this is a pretty heavy statement, it has

I have a lot of questions at this point: 
- *Giving that I could use LLMs a lot, how much should I use them though?* The most intuitive boudary for me is 
- *Tricky, it's tied to my feeling that I need to struggle to make something worth it. So how much should I struggle?* 
- *What does it mean to struggle in science? How much is it okay to struggle when thinking about science? What is an okay type of struggle?* The act of critically thinking about a subject and formulating informed opinions on it is objectively demanding in terms of mental energies. So at the very basis of doing science there is struggle.  

I think there is an important consideration to keep in mind in this case: our activity as scientists in inevitably and costantly intertwined with the activity of other scientists: think about having discussions with your colleagues over a cup of coffee, or group meetings, or presenting your work at conferences, or peer-reviewing other people's work. Sometimes, it happens that we find ourselves to do science alone; it can happen for many reasons, and for a variable amount of time. Personally, I will try my best to prioritise exercising the muscle of discussion with others, lest quickly running on the path of burn-out again.
Doing science alone is not doing science (?). 

So we need to keep in mind that using tools such as LLMs in our work will impact ourselves *and* other people, not necessarily in a negative way, but especially under certain circumstances (e.g. peer-reviewing papers), should be done with extra care.



## Outro

Much less intellectually than the first post, my latest activities include me playing a lot of Pokémon. I think I caught some rare ones (like a pink roundish one holding an egg...Chansey?), but I don't really know. They're just cute and I'm having fun.

I had much less fun in the past few days though, when I found myself playing real life Pokémon trying to catch some giant cockroaches running around my bedroom. 

Distract me from cockroaches and send me your thoughts on the topic, Pokémon facts and venting paragraphs to [luna.pianesi@gmail.com](mailto:luna.pianesi@gmail.com).

Thanks for reading!

![trapped-creature](/images/bleah.jpeg){: width="250" }

*Visual testimony of cockroach in shoe, shoe in trash bag, trash bag in bin, bin under 6L bottle. Huge shoutout to my flatmate who helped me catch them at 3 a.m. in the night.*


## References

[^execdys]: https://en.wikipedia.org/wiki/Executive_dysfunction

[^nature1]: https://www.nature.com/articles/d41586-025-01839-w

[^hacker]: Fant, D. and Milani, C. (2024). *Pedagogia Hacker* (Hacker pedagogy, Ch. 5, pp. 76-78). Elèuthera. 

[^women]: For instance: Timnit Gebru for ethical AI, Sasha Luccioni for AI's environmental impact